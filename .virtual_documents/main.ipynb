





# Import needed modules
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
import warnings 
from scipy.stats import ttest_ind
from sklearn import tree, ensemble
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV, KFold
from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix
warnings.filterwarnings('ignore', category=FutureWarning)








# Read in the dataset and set the patient ID as the index
data = pd.read_csv('Healthcare-Diabetes.csv', index_col= 'Id')
data.head(10)


# Check for missing values 
data.isna().any()





# Determine zero counts for each column where a value of zero is not feasible
(data.iloc[: ,1:8] == 0).sum()





# For data analysis, create DataFrames for each variable against outcome,
# removing zero values to assess trends via violin plots
        
dataframes = {} 

# Generating individual dataframes for each variable containing the variable and outcome
for col in data:
    if col == 'Pregnancies':
        dataframes[col] = data.loc[:, (col, 'Outcome')]
    if col == 'Outcome':
        pass
    # Remove zero value rows for all variables except pregancies and outcome
    else:
        dataframes[col] = data.loc[:, (col, 'Outcome')].query('`{}` != 0'.format(col))

# Create a variable to hold the desired palette colors for graphs
colors = ['skyblue','indianred']





plt.figure(3, figsize = (8,6), clear = True)

subplot_num = 1

for key in dataframes:
    plt.subplot(2, 4, subplot_num)
    plt.title(f"{key}", size = 10)
    sns.violinplot(data = dataframes[key], 
                   y = key, 
                   x = 'Outcome', 
                   alpha = 0.8,
                   palette = ['skyblue', 'skyblue']
                  )
    plt.xlabel('Diabetes')
    plt.xticks([0,1])
    subplot_num +=1 

plt.tight_layout()








# Store the numerical feature names to test
features = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin', 'BMI', 'DiabetesPedigreeFunction','Age']

# Store t-test results
t_test_results = []

# Loop through each feature to perform the t-tests
for feature in features:
    group_negative = dataframes[feature][dataframes[feature]['Outcome'] == 0][feature]
    group_positive = dataframes[feature][dataframes[feature]['Outcome'] == 1][feature]
    mean_diff = np.abs(group_negative.mean()-group_positive.mean())

    # Perform t-test
    t_stat, p_val = ttest_ind(group_negative, group_positive, axis = 0, equal_var = False)

    # Store results
    t_test_results.append({
        'Feature': feature,
        'Absolute Mean Difference': mean_diff,
        'T-Statistics': round(t_stat, 3),
        'P-value': p_val,
        'Significant': p_val < 0.05
    })
t_test_df = pd.DataFrame(t_test_results)
t_test_df








# Remove features from working dataset with substantial missing values: Insulin, Skin Thickness, BMI
cleaned_data = data.query('Glucose !=0 & BloodPressure !=0 & BMI !=0').copy()
cleaned_data.drop(['Insulin','SkinThickness'], axis = 1, inplace = True)
cleaned_data





# Count the number of participants in the cleaned sample
cleaned_data.shape





# Plot the Age Distribution of participants in a subplot
plt.figure(1, figsize = (12,6), clear = True)
plt.tight_layout()
plt.subplot(1,3,1)
sns.histplot(cleaned_data, 
             x = 'Age',
             bins = 18,
             color = 'skyblue')
plt.title('Age of Participants in Dataset')

# Plot the BMI distribution of Participants in a sub plot
plt.subplot(1,3,2)
sns.histplot(cleaned_data, 
             x = 'BMI',
             bins = 25,
             color = 'skyblue')
plt.title('BMI of Participants in Dataset')

# Plot the Distribution of the number of pregnancies in a subplot
plt.subplot(1,3,3)
sns.histplot(cleaned_data, 
              x = 'Pregnancies',
              bins = 18,
              color = 'skyblue')
plt.title('Number of Pregnancies of Participants in Dataset')

plt.tight_layout();








# Plot the number of participants with diabetes
plt.figure(2, figsize = (12,6), clear = True)
plt.subplot(1,2,1)
sns.histplot(cleaned_data, 
              x = 'Outcome',
              bins = 2,
              color = 'skyblue')
plt.title('Diagnosis Count Among Participants')
plt.xlabel('Diabetes')
plt.xticks([0.25, .75], ['Negative','Positive'])

# Plot glucose levels separated and color coded by Diabestes Diagnosis
plt.subplot(1,2,2)
sns.kdeplot(cleaned_data,
             x = 'Glucose',
             hue = 'Outcome',
             palette = colors)
plt.legend(title = 'Diabetes',
           labels = ['Negative','Positive'])
plt.title('2-Hour Plasma Glucose by Diabetes Diagnosis')
plt.tight_layout()





# Graph various variables vs. glucose
plt.figure(4, figsize = (8,12), clear = True)
plt.subplot(3,1, 1)

# Create plot one of the subplot
sns.scatterplot(x = 'BMI',
           y = 'Glucose',
           data = cleaned_data,
           hue = 'Outcome',
           palette = colors
               )
plt.legend(title = 'Diabetes',
            labels = ['Negative', ' Positive'] )

# Edit the titles of the first subplot
plt.title('BMI vs. 2-Hour Plasma Glucose')
plt.xlabel('BMI')
plt.ylabel('2-Hour Plasma Glucose')

# Create plot two of the subplot
plt.subplot(3,1, 2)
sns.scatterplot(x = 'BloodPressure',
           y = 'Glucose',
           data = cleaned_data,
           hue = 'Outcome',
           palette = colors,
            )
plt.legend(title = 'Diabetes',
            labels = ['Negative', ' Positive'] )
# Edit the titles of the second subplot
plt.title('Blood Pressure vs 2-Hour Plasma Glucose')
plt.xlabel('Diastolic Blood Pressure')
plt.ylabel('2-Hour Plasma Glucose')

# Create plot three of the subplot
plt.subplot(3,1, 3)
sns.scatterplot(x = 'DiabetesPedigreeFunction',
           y = 'Glucose',
           data = cleaned_data,
           hue = 'Outcome',
           palette = colors
               )
plt.legend(title = 'Diabetes',
            labels = ['Negative', ' Positive'] )

# Edit the titles of the third subplot
plt.title('Diabetes Pedigree Function vs 2-Hour Plasma Glucose')
plt.xlabel('Diabetes Pedigree Function')
plt.ylabel('2-Hour Plasma Glucose')
plt.tight_layout()












# Create a new dataset of only rows of pregnancies
pregnancies = cleaned_data[cleaned_data['Pregnancies']>0]
# Count the number of rows in this data set
pregnancies.shape





# Plot the number of pregnancies separated and color coded by Diabetes Diagnosis
plt.figure(5, figsize = (8,6), clear = True)
plt.subplot(1, 2, 1)
plt.title('Pregnancies by Diabetes Diagnosis', size = 10)
sns.violinplot(data = pregnancies, 
                   y = 'Pregnancies', 
                   x = 'Outcome', 
                   alpha = 0.8,
                   palette = ['skyblue', 'skyblue']
                  )
plt.xlabel('Diabetes')
plt.xticks([0,1],['Negative','Positive'])
    
# Plot pregnancies on a kde plot
plt.subplot(1,2,2)
sns.kdeplot(pregnancies,
             x = 'Pregnancies',
             hue = 'Outcome',
             palette = colors)
plt.legend(title = 'Diabetes',
           labels = ['Negative','Positive'])
plt.title('Number of Pregnancies by Diabetes Diagnosis')
plt.tight_layout()








# Split the data into train and test datasets 
x_train, x_test, y_train, y_test = train_test_split(
    cleaned_data.drop(columns = 'Outcome', axis = 1),
    cleaned_data.Outcome,
    test_size = 0.3, 
    random_state = 3870)
x_train


# Form the grid of values to choose hyper parameters from 
hps_grid = {
    'n_estimators': [10, 50, 100, 200, 300, 400, 500],
    'max_features': np.arange(1,7, 1).tolist(),
    'max_depth': [1, 2, 10, 20, 50, None]
}

hps_grid


# Step 2) Define a collection of random forest for each combination of the hyper parameter choices in the grid

# 5-fold cv partition
cv5 = KFold(n_splits = 5, shuffle = True, random_state = 3870)

# Forming the empty trees for each combo of parameters in hyper_grid
hp_grid_search = GridSearchCV(
    ensemble.RandomForestClassifier(random_state = 3870),
    hps_grid,
    cv = cv5,
    scoring = 'r2',
    n_jobs = -1
)


# Fit the random forests created to the training data
hp_grid_search.fit(X = x_train, y = y_train)


print(f"The best choice for the number of trees is {hp_grid_search.best_params_['n_estimators']}")
print(f"The best choice for the number of features is {hp_grid_search.best_params_['max_features']}")
print(f"The best choice for max depth is {hp_grid_search.best_params_['max_depth']}")
print(f"which has an R-squared of: {hp_grid_search.best_score_: .4f}")


best_model = hp_grid_search.best_estimator_
best_model





# Accuracy of best model with train and test sets
rf_training_accuracy = best_model.score(x_train, y_train)
rf_testing_accuracy = best_model.score(x_test, y_test)
print('Training Accuracy: ',round(rf_training_accuracy, 3), ' Testing Accuracy: ',round(rf_testing_accuracy, 3))


# Determine the predicted y values from the best model
y_pred = best_model.predict(X=x_test)


# Create a confusion matrix with the percent accurately predicted 
random_forest_confusion_matrix = confusion_matrix(
    y_true = y_test,
    y_pred = y_pred, 
    normalize = 'true'
)


# Display the confusion matrix
sns.heatmap(random_forest_confusion_matrix,
           annot = True,
           fmt = '.3f',
           cmap = 'Blues',
           annot_kws = {'size': 16})
plt.xlabel("Predicted")
plt.ylabel("Actual");

















xgb_hp_dict = {
    'learning_rate':    [0.01, 0.05, 0.1, 0.2, 0.3], # Each new prediction's contribution to y_hat
    'n_estimators':     [100, 250, 500, 750, 1000],  # Number of trees to make
    'max_depth':        [1, 3, 5, 7],                # Maximum number of splits: 1, 8, 32, 128 leaf nodes   
    'colsample_bytree': [0.4, 0.6, 0.8, 1],          # Number of features to use per tree (40, 60, 80, & 100%)
    'lambda':           [0, 0.5, 1, 2, 5]            # Regularization parameters to try
}











# 5 fold cross-val
cv5 = KFold(n_splits = 5, shuffle = True, random_state = 3870)

# Defining the model:
xgb_class = XGBClassifier(random_state = 3870, early_stopping_rounds = 50)

# Defining the grid search
grid_search = GridSearchCV(
    estimator = xgb_class,
    param_grid = xgb_hp_dict,
    n_jobs = -1,
    cv = cv5    
)








gs_results = (
    grid_search
    .fit(
        x_train, y_train,
        eval_set = [(x_test, y_test)],
        verbose = False
    )
)


best_hps = grid_search.best_params_
best_hps


xgb_best_model = (
    XGBClassifier(
        **best_hps,  # Assigns the dictionary values to the parameter keys
        random_state = 3870,
        early_stopping_rounds = 20
    )
    .fit(
        x_train, y_train,
        eval_set = [(x_test, y_test)],
        verbose = False
    )
)

# Calculating the accuracy of the model
accuracy_xgb_tuned = best_model.score(x_test, y_test)
print('Train Accuracy: ',best_model.score(x_train, y_train), ' Test Accuracy: ', accuracy_xgb_tuned)





xgb_y_pred = best_model.predict(X = x_test)


# Create a normalized confusion matrix of true/ false positives and negatives
XGB_confusion_matrix = confusion_matrix(
    y_true = y_test,
    y_pred = xgb_y_pred, 
    normalize = 'true'
)


# Display the confusion matrix
sns.heatmap(XGB_confusion_matrix,
           annot = True,
           fmt = '.3f',
           cmap = 'Blues',
           annot_kws = {'size': 16})
plt.xlabel("Predicted")
plt.ylabel("Actual");






