





# Import needed modules
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
import warnings 
from scipy.stats import ttest_ind
from sklearn import tree, ensemble
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV, KFold
warnings.filterwarnings('ignore', category=FutureWarning)








# Read in the dataset and set the patient ID as the index
data = pd.read_csv('Healthcare-Diabetes.csv', index_col= 'Id')
data.head(10)


# Check for missing values 
data.isna().any()





# Determine zero counts for each column where a value of zero is not feasible
(data.iloc[: ,1:8] == 0).sum()





# For data analysis, create DataFrames for each variable against outcome,
# removing zero values to assess trends via violin plots
        
dataframes = {} 

# Generating individual dataframes for each variable containing the variable and outcome
for col in data:
    if col == 'Pregnancies':
        dataframes[col] = data.loc[:, (col, 'Outcome')]
    if col == 'Outcome':
        pass
    # Remove zero value rows for all variables except pregancies and outcome
    else:
        dataframes[col] = data.loc[:, (col, 'Outcome')].query('`{}` != 0'.format(col))

# Create a variable to hold the desired palette colors for graphs
colors = ['skyblue','indianred']





plt.figure(3, figsize = (8,6), clear = True)

subplot_num = 1

for key in dataframes:
    plt.subplot(2, 4, subplot_num)
    plt.title(f"{key}", size = 10)
    sns.violinplot(data = dataframes[key], 
                   y = key, 
                   x = 'Outcome', 
                   alpha = 0.8,
                   palette = ['skyblue', 'skyblue']
                  )
    plt.xlabel('Diabetes')
    plt.xticks([0,1])
    subplot_num +=1 

plt.tight_layout()








# Store the numerical feature names to test
features = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin', 'BMI', 'DiabetesPedigreeFunction','Age']

# Store t-test results
t_test_results = []

# Loop through each feature to perform the t-tests
for feature in features:
    group_negative = dataframes[feature][dataframes[feature]['Outcome'] == 0][feature]
    group_positive = dataframes[feature][dataframes[feature]['Outcome'] == 1][feature]
    mean_diff = np.abs(group_negative.mean()-group_positive.mean())

    # Perform t-test
    t_stat, p_val = ttest_ind(group_negative, group_positive, axis = 0, equal_var = False)

    # Store results
    t_test_results.append({
        'Feature': feature,
        'Absolute Mean Difference': mean_diff,
        'T-Statistics': round(t_stat, 3),
        'P-value': p_val,
        'Significant': p_val < 0.05
    })
t_test_df = pd.DataFrame(t_test_results)
t_test_df








# Remove features from working dataset with substantial missing values: Insulin, Skin Thickness, BMI
cleaned_data = data.query('Glucose !=0 & BloodPressure !=0 & BMI !=0').copy()
cleaned_data.drop(['Insulin','SkinThickness'], axis = 1, inplace = True)
cleaned_data





# Count the number of participants in the cleaned sample
cleaned_data.shape





# Plot the Age Distribution of participants in a subplot
plt.figure(1, figsize = (12,6), clear = True)
plt.tight_layout()
plt.subplot(1,3,1)
sns.histplot(cleaned_data, 
             x = 'Age',
             bins = 18,
             color = 'skyblue')
plt.title('Age of Participants in Dataset')

# Plot the BMI distribution of Participants in a sub plot
plt.subplot(1,3,2)
sns.histplot(cleaned_data, 
             x = 'BMI',
             bins = 25,
             color = 'skyblue')
plt.title('BMI of Participants in Dataset')

# Plot the Distribution of the number of pregnancies in a subplot
plt.subplot(1,3,3)
sns.histplot(cleaned_data, 
              x = 'Pregnancies',
              bins = 18,
              color = 'skyblue')
plt.title('Number of Pregnancies of Participants in Dataset')

plt.tight_layout();








# Plot the number of participants with diabetes
plt.figure(2, figsize = (12,6), clear = True)
plt.subplot(1,2,1)
sns.histplot(cleaned_data, 
              x = 'Outcome',
              bins = 2,
              color = 'skyblue')
plt.title('Diagnosis Count Among Participants')
plt.xlabel('Diabetes')
plt.xticks([0.25, .75], ['Negative','Positive'])

#plot insulin levels separated and color coded by Diabestes Diagnosis
plt.subplot(1,2,2)
sns.kdeplot(cleaned_data,
             x = 'Glucose',
             hue = 'Outcome',
             palette = colors)
plt.legend(title = 'Diabetes',
           labels = ['Negative','Positive'])
plt.title('2-Hour Insulin Serum by Diabetes Diagnosis')
plt.tight_layout()






#Graph various variables vs. glucose
plt.figure(4, figsize = (8,12), clear = True)
plt.subplot(3,1, 1)

#create plot one of the subplot
sns.scatterplot(x = 'BMI',
           y = 'Glucose',
           data = cleaned_data,
           hue = 'Outcome',
           palette = colors
               )
plt.legend(title = 'Diabetes',
            labels = ['Negative', ' Positive'] )

#edit the titles of the first subplot
plt.title('BMI vs. 2-Hour Plasma Glucose')
plt.xlabel('BMI')
plt.ylabel('2-Hour Plasma Glucose')

#Create plot two of the subplot
plt.subplot(3,1, 2)
sns.scatterplot(x = 'BloodPressure',
           y = 'Glucose',
           data = cleaned_data,
           hue = 'Outcome',
           palette = colors,
            )
plt.legend(title = 'Diabetes',
            labels = ['Negative', ' Positive'] )
#edit the titles of the second subplot
plt.title('Blood Pressure vs 2-Hour Plasma Glucose')
plt.xlabel('Diastolic Blood Pressure')
plt.ylabel('2-Hour Plasma Glucose')

#create plot three of the subplot
plt.subplot(3,1, 3)
sns.scatterplot(x = 'DiabetesPedigreeFunction',
           y = 'Glucose',
           data = cleaned_data,
           hue = 'Outcome',
           palette = colors
               )
plt.legend(title = 'Diabetes',
            labels = ['Negative', ' Positive'] )

#edit the titles of the third subplot
plt.title('Diabetes Pedigree Function vs 2-Hour Plasma Glucose')
plt.xlabel('Diabetes Pedigree Function')
plt.ylabel('2-Hour Plasma Glucose')
plt.tight_layout()












#create a new dataset of only rows of pregnancies
pregnancies = cleaned_data[cleaned_data['Pregnancies']>0]
#count the number of rows in this data set
pregnancies.shape





#plot the number of pregnancies separated and color coded by Diabetes Diagnosis
plt.figure(5, figsize = (8,6), clear = True)
plt.subplot(1, 2, 1)
plt.title('Pregnancies by Diabetes Diagnosis', size = 10)
sns.violinplot(data = pregnancies, 
                   y = 'Pregnancies', 
                   x = 'Outcome', 
                   alpha = 0.8,
                   palette = ['skyblue', 'skyblue']
                  )
plt.xlabel('Diabetes')
plt.xticks([0,1],['Negative','Positive'])
    
#plot pregnancies on a kde plot
plt.subplot(1,2,2)
sns.kdeplot(pregnancies,
             x = 'Pregnancies',
             hue = 'Outcome',
             palette = colors)
plt.legend(title = 'Diabetes',
           labels = ['Negative','Positive'])
plt.title('Number of Pregnancies by Diabetes Diagnosis')
plt.tight_layout()








# Split the data into train and test datasets 
x_train, x_test, y_train, y_test = train_test_split(
    cleaned_data.drop(columns = 'Outcome', axis = 1),
    cleaned_data.Outcome,
    test_size = 0.3, 
    random_state = 3870)
x_train


# Form the grid of values to choose hyper parameters from 
hps_grid = {
    'n_estimators': [10, 50, 100, 200, 300, 400, 500],
    'max_features': np.arange(1,7, 1).tolist(),
    'max_depth': [1, 2, 10, 20, 50, 100, None]
}

hps_grid


# Step 2) Define a collection of random forest for each combination of the hyper parameter choices in the grid

# 5-fold cv partition
cv5 = KFold(n_splits = 5, shuffle = True, random_state = 3870)

# Forming the empty trees for each combo of parameters in hyper_grid
hp_grid_search = GridSearchCV(
    ensemble.RandomForestRegressor(random_state = 3870),
    hps_grid,
    cv = cv5,
    scoring = 'r2',
    n_jobs = -1
)


# Fit the random forests created to the training data
hp_grid_search.fit(X = x_train, y = y_train)


print(f"The best choice for the number of trees is {hp_grid_search.best_params_['n_estimators']}")
print(f"The best choice for the number of features is {hp_grid_search.best_params_['max_features']}")
print(f"The best choice for max depth is {hp_grid_search.best_params_['max_depth']}")
print(f"which has an R-squared of: {hp_grid_search.best_score_: .4f}")


best_model = hp_grid_search.best_estimator_
best_model



