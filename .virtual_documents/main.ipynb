





#import needed modules
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
import warnings 
from scipy.stats import ttest_ind
from sklearn import tree, ensemble
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV, KFold
from xgboost import XGBRegressor
warnings.filterwarnings('ignore', category=FutureWarning)








# Read in the dataset and set the patient ID as the index
data = pd.read_csv('Healthcare-Diabetes.csv', index_col= 'Id')
data.head(10)


#check for na 
data.isna().any()





# See how many zero counts for each column that shouldn't contain zeros
(data.iloc[: ,1:8] == 0).sum()





# For data analysis, create DataFrames for each variable against outcome,
# removing zero values to assess trends via violin plots
        
dataframes = {} 

#generating individual dataframes for each variable containing the variable and outcome
for col in data:
    if col == 'Pregnancies':
        dataframes[col] = data.loc[:, (col, 'Outcome')]
    if col == 'Outcome':
        pass
    #remove zero value rows for all variables except pregancies and outcome
    else:
        dataframes[col] = data.loc[:, (col, 'Outcome')].query('`{}` != 0'.format(col))

#create a variable to hold the desired palette colors for graphs
colors = ['skyblue','indianred']





plt.figure(3, figsize = (8,6), clear = True)

subplot_num = 1

for key in dataframes:
    plt.subplot(2, 4, subplot_num)
    plt.title(f"{key}", size = 10)
    sns.violinplot(data = dataframes[key], 
                   y = key, 
                   x = 'Outcome', 
                   alpha = 0.8,
                   palette = ['skyblue', 'skyblue']
                  )
    plt.xlabel('Diabetes')
    plt.xticks([0,1])
    subplot_num +=1 

plt.tight_layout()








#store the numerical feature names to test
features = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin', 'BMI', 'DiabetesPedigreeFunction']

#store t-test results
t_test_results = []
#Loop through each feature to perform the t-tests
for feature in features:
    group_negative = dataframes[feature][dataframes[feature]['Outcome'] == 0][feature]
    group_positive = dataframes[feature][dataframes[feature]['Outcome'] == 1][feature]

    #do the t-test
    t_stat, p_val = ttest_ind(group_negative, group_positive, axis = 0, equal_var = False)

    #store the results
    t_test_results.append({
        'Feature': feature,
        'T-Statistics': round(t_stat, 3),
        'P-value': p_val,
        'Significant': p_val < 0.05
    })
t_test_df = pd.DataFrame(t_test_results)
t_test_df














# cleaning the entire dataframe to remove zero values rather than individual variables
cleaned_data = data.query('Glucose !=0 & BloodPressure !=0').copy()
cleaned_data.drop(['Insulin','SkinThickness'], axis = 1, inplace = True)
cleaned_data





#Count the number of participants in the cleaned sample
cleaned_data.shape





#Plot the Age Distribution of participants in a subplot
plt.figure(1, figsize = (12,6), clear = True)
plt.tight_layout()
plt.subplot(1,3,1)
sns.histplot(cleaned_data, 
             x = 'Age',
             bins = 18,
             color = 'skyblue')
plt.title('Age of Participants in Dataset')

#plot the BMI distribution of Participants in a sub plot
plt.subplot(1,3,2)
sns.histplot(cleaned_data, 
             x = 'BMI',
             bins = 25,
             color = 'skyblue')
plt.title('BMI of Participants in Dataset')

#plot the Distribution of the number of pregnancies in a subplot
plt.subplot(1,3,3)
sns.histplot(cleaned_data, 
              x = 'Pregnancies',
              bins = 18,
              color = 'skyblue')
plt.title('Number of Pregnancies of Participants in Dataset')

plt.tight_layout()





#Plot the number of participants with diabetes
plt.figure(2, figsize = (12,6), clear = True)
plt.subplot(1,2,1)
sns.histplot(cleaned_data, 
              x = 'Outcome',
              bins = 2,
              color = 'skyblue')
plt.title('Diagnosis Count Among Participants')
plt.xlabel('Diabetes')
plt.xticks([0.25, .75], ['Negative','Positive'])

#plot insulin levels separated and color coded by Diabestes Diagnosis
plt.subplot(1,2,2)
sns.kdeplot(cleaned_data,
             x = 'Glucose',
             hue = 'Outcome',
             palette = colors)
plt.legend(title = 'Diabetes',
           labels = ['Negative','Positive'])
plt.title('2-Hour Plasma Glucose by Diabetes Diagnosis')
plt.tight_layout()






#Graph various variables vs. glucose
plt.figure(4, figsize = (8,12), clear = True)
plt.subplot(3,1, 1)

#create plot one of the subplot
sns.scatterplot(x = 'BMI',
           y = 'Glucose',
           data = cleaned_data,
           hue = 'Outcome',
           palette = colors
               )
plt.legend(title = 'Diabetes',
            labels = ['Negative', ' Positive'] )

#edit the titles of the first subplot
plt.title('BMI vs. 2-Hour Plasma Glucose')
plt.xlabel('BMI')
plt.ylabel('2-Hour Plasma Glucose')

#Create plot two of the subplot
plt.subplot(3,1, 2)
sns.scatterplot(x = 'BloodPressure',
           y = 'Glucose',
           data = cleaned_data,
           hue = 'Outcome',
           palette = colors,
            )
plt.legend(title = 'Diabetes',
            labels = ['Negative', ' Positive'] )
#edit the titles of the second subplot
plt.title('Blood Pressure vs 2-Hour Plasma Glucose')
plt.xlabel('Diastolic Blood Pressure')
plt.ylabel('2-Hour Plasma Glucose')

#create plot three of the subplot
plt.subplot(3,1, 3)
sns.scatterplot(x = 'DiabetesPedigreeFunction',
           y = 'Glucose',
           data = cleaned_data,
           hue = 'Outcome',
           palette = colors
               )
plt.legend(title = 'Diabetes',
            labels = ['Negative', ' Positive'] )

#edit the titles of the third subplot
plt.title('Diabetes Pedigree Function vs 2-Hour Plasma Glucose')
plt.xlabel('Diabetes Pedigree Function')
plt.ylabel('2-Hour Plasma Glucose')
plt.tight_layout()












#create a new dataset of only rows of pregnancies
pregnancies = cleaned_data[cleaned_data['Pregnancies']>0]
#count the number of rows in this data set
pregnancies.shape





#plot the number of pregnancies separated and color coded by Diabetes Diagnosis
plt.figure(5, figsize = (8,6), clear = True)
plt.subplot(1, 2, 1)
plt.title('Pregnancies by Diabetes Diagnosis', size = 10)
sns.violinplot(data = pregnancies, 
                   y = 'Pregnancies', 
                   x = 'Outcome', 
                   alpha = 0.8,
                   palette = ['skyblue', 'skyblue']
                  )
plt.xlabel('Diabetes')
plt.xticks([0,1],['Negative','Positive'])
    
#plot pregnancies on a kde plot
plt.subplot(1,2,2)
sns.kdeplot(pregnancies,
             x = 'Pregnancies',
             hue = 'Outcome',
             palette = colors)
plt.legend(title = 'Diabetes',
           labels = ['Negative','Positive'])
plt.title('Number of Pregnancies by Diabetes Diagnosis')
plt.tight_layout()








# Split the data into train and test datasets 
x_train, x_test, y_train, y_test = train_test_split(
    cleaned_data.drop(columns = 'Outcome', axis = 1),
    cleaned_data.Outcome,
    test_size = 0.3, 
    random_state = 3870)
x_train


# Form the grid of values to choose hyper parameters from 
hps_grid = {
    'n_estimators': [10, 50, 100, 200, 300, 400, 500],
    'max_features': np.arange(1,7, 1).tolist(),
    'max_depth': [1, 2, 10, 20, 50, None]
}

hps_grid


# Step 2) Define a collection of random forest for each combination of the hyper parameter choices in the grid

# 5-fold cv partition
cv5 = KFold(n_splits = 5, shuffle = True, random_state = 3870)

# Forming the empty trees for each combo of parameters in hyper_grid
hp_grid_search = GridSearchCV(
    ensemble.RandomForestClassifier(random_state = 3870),
    hps_grid,
    cv = cv5,
    scoring = 'r2',
    n_jobs = -1
)


# Fit the random forests created to the training data
hp_grid_search.fit(X = x_train, y = y_train)


print(f"The best choice for the number of trees is {hp_grid_search.best_params_['n_estimators']}")
print(f"The best choice for the number of features is {hp_grid_search.best_params_['max_features']}")
print(f"The best choice for max depth is {hp_grid_search.best_params_['max_depth']}")
print(f"which has an R-squared of: {hp_grid_search.best_score_: .4f}")


best_model = hp_grid_search.best_estimator_
best_model





grid_search_df = pd.DataFrame(hp_grid_search.cv_results_['params'])
grid_search_df['R2'] = hp_grid_search.cv_results_['mean_test_score']
grid_search_df['max_depth'] = (
    grid_search_df['max_depth'].apply(
        lambda x:np.where(pd.isna(x), 'Unlimited', str(x))
    )
)
grid_search_df.sort_values('n_estimators', inplace = True)
grid_search_df


# Making the empty faceted graph
hp_gs_plot = sns.FacetGrid(
    grid_search_df,
    col = 'max_depth',
    col_wrap = 3,
    sharey = False
)

# Adding the lines to the facets
hp_gs_plot.map_dataframe(
    sns.lineplot,          # Graph to make
    x = 'n_estimators',
    y = 'R2',
    hue = 'max_features'
)

# Adding a legend
hp_gs_plot.add_legend(title = 'Features\nPer Tree')

# Changing the x & y-axes
hp_gs_plot.set_axis_labels('Number of Trees per Forest', r'$R^2$');


xgb_hp_dict = {
    'learning_rate':    [0.01, 0.05, 0.1, 0.2, 0.3], # Each new prediction's contribution to y_hat
    'n_estimators':     [100, 250, 500, 750, 1000],  # Number of trees to make
    'max_depth':        [1, 3, 5, 7],                # Maximum number of splits: 1, 8, 32, 128 leaf nodes   
    'colsample_bytree': [0.4, 0.6, 0.8, 1],          # Number of features to use per tree (40, 60, 80, & 100%)
    'lambda':           [0, 0.5, 1, 2, 5]            # Regularization parameters to try
}


# 5 fold cross-val
cv5 = KFold(n_splits = 5, shuffle = True, random_state = 3870)

# Defining the model:
xgb_reg = XGBClassifier(random_state = 3870, early_stopping_rounds = 50)

# Defining the grid search
grid_search = GridSearchCV(
    estimator = xgb_reg,
    param_grid = xgb_hp_dict,
    n_jobs = -1,
    cv = cv5    
)


gs_results = (
    grid_search
    .fit(
        x_train, y_train,
        eval_set = [(x_test, y_test)],
        verbose = False
    )
)


best_hps = grid_search.best_params_
best_hps


best_model = (
    XGBRegressor(
        **best_hps,  # Assigns the dictionary values to the parameter keys
        random_state = 3870,
        early_stopping_rounds = 20
    )
    .fit(
        x_train, y_train,
        eval_set = [(x_test, y_test)],
        verbose = False
    )
)

# Calculating the R2
R2_xgb_tuned = best_model.score(x_test, y_test)
R2_xgb_tuned



