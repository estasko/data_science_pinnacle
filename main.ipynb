{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04a79f1c-d070-407b-8527-afe08394a51e",
   "metadata": {},
   "source": [
    "# Predicting Diabetes Diagnosis Using Health Factors\n",
    "Data Science 3870 <br> Dr. Jacob Martin<br>Authors: Abby Howard, Arai Lubas, Ella Stasko\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "We are interested in exploring what combination of the collected features best predicts whether an individual has diabetes. Given these inputs, we want to determine whether we can create a model to predict this accurately.\n",
    "\n",
    "Our team will use the Healthcare Diabetes Dataset from Kaggle. Each instance represents various aspects of an individualâ€™s health data and their diagnosis or non-diagnosis of diabetes. It was last updated in 2023, meaning the data is recent and relevant. The dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK). There are 2,768 patients included in the data, with 10 features recorded. Each instance represents a unique individual with recorded features listed below:\n",
    "\n",
    "1.\t**Id**: Unique identifier for each data entry.\n",
    "2. **Pregnancies**: Number of times pregnant. (Includes people who cannot get pregnant).\n",
    "4.\t**Glucose**: Plasma glucose concentration over 2 hours in an oral glucose tolerance test.\n",
    "5.\t**BloodPressure**: Diastolic blood pressure (mm Hg).\n",
    "6.\t**SkinThickness**: Triceps skinfold thickness (mm). Indicator of body fat distribution.\n",
    "7.\t**Insulin**: 2-Hour serum insulin (mu U/ml).\n",
    "8.\t**BMI**: Body mass index (weight in kg / height in m^2).\n",
    "9.\t**DiabetesPedigreeFunction**: Diabetes pedigree function, a genetic score of diabetes.\n",
    "10.\t**Age**: Age in years.\n",
    "11.\t**Outcome**: Binary classification indicating the presence (1) or absence (0) of diabetes.\n",
    "   \n",
    "Features we are particularly interested in to predict whether the individual has diabetes are glucose, diabetes pedigree function, and insulin features. We are surprised by the skin thickness and intrigued to determine whether this has any predictive value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057238db-24ce-4c6a-889b-94f96deaf847",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "Before creating our model to predict diabetes diagnosis, we will evaluate the dataset through visualization. It is important to explore data features to identify any abnormalities in the data. First we need to read in our data and confirm there are no missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9777c2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings \n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn import tree, ensemble\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b7575",
   "metadata": {
    "lines_to_next_cell": 0,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in the dataset and set the patient ID as the index\n",
    "data = pd.read_csv('Healthcare-Diabetes.csv', index_col= 'Id')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10e4d18-ce89-4ead-a486-34d9688b9401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values \n",
    "data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64232cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for zero values\n",
    "(data.iloc[: ,1:8] == 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ae175b-5471-44d7-b7ef-05f574a297cb",
   "metadata": {},
   "source": [
    "The data does not contain any missing values however, we noticed that there are multiple observations with zero values recorded for features that cannot equal zero (age, BMI, Insulin etc.)\n",
    "\n",
    "We will assume that for features: Glucose, Blood Pressure, Skin Thickness, Insulin, BMI, Diabetes Pedigree Function, and Age, values of zero are improbable and thus are missing values that need to be removed. We include further analysis to determine how to approach this problem. Features such as Insulin and Skin thickness have a large number of zeros and in order to determine whether these features should be removed from the dataset to preserve sample size or kept at the expense of a smaller sample size, we will perfom individual analysis of each feature against outcome with all zero values removed.\n",
    "\n",
    "First we create DataFrames of each variable against outcome removing all zero values. We then plot the Age, BMI, and Pregnancies distributions of the data to start gain an understanding of the data sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c909be95",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Create DataFrames for each variable against outcome removing zero values to assess trends via violin plots\n",
    "dataframes = {} \n",
    "\n",
    "# Generating individual dataframes for each variable containing the variable and outcome\n",
    "for col in data:\n",
    "    if col == 'Pregnancies':\n",
    "        dataframes[col] = data.loc[:, (col, 'Outcome')]\n",
    "    if col == 'Outcome':\n",
    "        pass\n",
    "    # Remove zero value rows for all variables except pregancies and outcome\n",
    "    else:\n",
    "        dataframes[col] = data.loc[:, (col, 'Outcome')].query('`{}` != 0'.format(col))\n",
    "\n",
    "# Create a variable to hold the desired palette colors for graphs\n",
    "colors = ['skyblue','palevioletred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ad98c-2c92-46bc-b8ba-8d8fb1b53d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Age Distribution of participants in a subplot\n",
    "plt.figure(1, figsize = (12,6), clear = True)\n",
    "plt.tight_layout()\n",
    "plt.subplot(1,3,1)\n",
    "sns.histplot(dataframes['Age'], \n",
    "             x = 'Age',\n",
    "             bins = 18,\n",
    "             color = 'skyblue')\n",
    "plt.title('Age of Participants in Dataset')\n",
    "\n",
    "# Plot the BMI distribution of Participants in a sub plot\n",
    "plt.subplot(1,3,2)\n",
    "sns.histplot(dataframes['BMI'], \n",
    "             x = 'BMI',\n",
    "             bins = 25,\n",
    "             color = 'skyblue')\n",
    "plt.title('BMI of Participants in Dataset')\n",
    "\n",
    "# Plot the Distribution of the number of pregnancies in a subplot\n",
    "plt.subplot(1,3,3)\n",
    "sns.histplot(dataframes['Pregnancies'], \n",
    "              x = 'Pregnancies',\n",
    "              bins = 18,\n",
    "              color = 'skyblue')\n",
    "plt.title('Number of Pregnancies of Participants in Dataset')\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc8853c-00fa-4bee-9f9f-f7568b14fb05",
   "metadata": {},
   "source": [
    "From this we see the overall age of the sample is rightly skewed, the overall BMI of the population is normally distributed with a slight right skew and the number of pregnancies in the data is rightly skewed. \n",
    "\n",
    "We next isolated each feature and grouped the values by outcome. This allows us to visualize and potentially identify any obvious intra-variable distribution difference against the outcome, diabetes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abfc252",
   "metadata": {
    "title": "Plot Violin Plots"
   },
   "outputs": [],
   "source": [
    "# Create figure\n",
    "plt.figure(1, figsize = (8,6), clear = True)\n",
    "subplot_num = 1\n",
    "\n",
    "# Loop through features and plot separated by outcome\n",
    "for key in dataframes:\n",
    "    plt.subplot(2, 4, subplot_num)\n",
    "    plt.title(f\"{key}\", size = 10)\n",
    "    sns.violinplot(data = dataframes[key], \n",
    "                   y = key, \n",
    "                   x = 'Outcome', \n",
    "                   alpha = 0.8,\n",
    "                   palette =  ['skyblue', 'skyblue']\n",
    "                  )\n",
    "    plt.xlabel('Diabetes')\n",
    "    plt.xticks([0,1])\n",
    "    subplot_num +=1 \n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a965659-63dd-4580-801d-1f5aed806cfb",
   "metadata": {},
   "source": [
    "We evaluate each feature by comparing the distribution of its values against outcome \n",
    "\n",
    "\n",
    "**Pregnancies**: It appears that patients with diabetes have a wider distribution of number of pregnancies and higher med. This becomes hard to interpret as the data -nor its metadata file / documentation- do not specify sex and thus may include patients unable to have children. This might jepordize the valididity of using this feature. Alternatively, as sex is not a feature of this dataset it may not impact diabetes outcome. Thus, pregnancy impact on diabetes outcome could easily be determine by pregnancy or no pregnancy - where a person with or without the ability to get pregnant, who has not been pregrnant, has the same chance of a positive outcome. Regardless, we will perform further analysis of this feature. \n",
    "\n",
    "**Glucose**: Patients with diabetes have on average a higher blood glucose level than those without, this is a possible feature for prediction as the distributions appear distinct from one another.\n",
    "\n",
    "**Blood Pressure**: Patients with diabetes have a slighly higher median diastolic blood pressure than those without, but overall the distributions appear similar.\n",
    "\n",
    "**Skin Thickeness**: Patients with diabetes have a slighly higher median skin thickness than those without, but overall the distributions appear similar. This is a feature with a high missing rate (800 values) and the minimal difference in distribution and median begins to suggest removal of this feature would be acceptable. \n",
    "\n",
    "**Insulin**: Patients with diabetes have a higher median 2-hour serum insulin than those without. This feature has a high missing rate (1330 values) and the significant difference in median insulin value suggests it might impact the effectiveness of prediction. This further solidifies that more evaluation of the insulin feature is needed before its removal. \n",
    "\n",
    "**BMI**: Patients with diabetes have a higher median BMI and patients without diabetes follow a wider distribution while patients with diabetes follow a narrow, normal distribution. \n",
    "\n",
    "**Diabetes Pedigree Function**: Patients with and without diabetes follow a similar right skewed distribution of diabetes pedigree function with comparable median values.\n",
    "\n",
    "**Age**: Patients with diabetes are, on average, older than those without and patients with diabetes have a much wider distribution than those without.\n",
    "\n",
    "\n",
    "\n",
    "### Summary Statistics\n",
    "\n",
    "To further examine the relationships between the variables and diabetes diagnoses, we performed two-sample independent t-tests to evaluate the differences in means between the groups and whether or not they are statistically significant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0bdf51-9c70-4a5f-b1e6-7e63b3c721be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the numerical feature names to test\n",
    "features = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin', 'BMI', 'DiabetesPedigreeFunction','Age']\n",
    "\n",
    "# Store t-test results\n",
    "t_test_results = []\n",
    "\n",
    "# Loop through each feature to perform the t-tests\n",
    "for feature in features:\n",
    "    group_negative = dataframes[feature][dataframes[feature]['Outcome'] == 0][feature]\n",
    "    group_positive = dataframes[feature][dataframes[feature]['Outcome'] == 1][feature]\n",
    "    mean_diff = np.abs(group_negative.mean()-group_positive.mean())\n",
    "\n",
    "    # Perform t-test\n",
    "    t_stat, p_val = ttest_ind(group_negative, group_positive, axis = 0, equal_var = False)\n",
    "\n",
    "    # Store results\n",
    "    t_test_results.append({\n",
    "        'Feature': feature,\n",
    "        'Absolute Mean Difference': mean_diff,\n",
    "        'T-Statistics': round(t_stat, 3),\n",
    "        'P-value': p_val,\n",
    "        'Significant': p_val < 0.05\n",
    "    })\n",
    "t_test_df = pd.DataFrame(t_test_results)\n",
    "t_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8938d6f1-d094-45c5-81ad-91cfd835466f",
   "metadata": {},
   "source": [
    "All features have a significant p-value. While only the means of Glucose and Insulin are drastically different, because the sample sizes are all large it creates large test statistics and small p-values. P-Values are often ignored in clinical work because it is more clinically relevant to look at the magnitude of the difference of the relevant metric. With a large enough sample size even small changes that are not clinically relevant could be statistially significant. As seen above the mean difference of Blood Pressure between patients with and without diabetes is ~4.6; while this is statistically significant, a difference in Blood Pressure of 4.6mmHg is not clinically relevant. However we can not anticipate which features will be the  useful in predicting diabetes using this information. Yet, Glucose and Insulin have a more relevant magnitude of difference in their mean values across outcomes indicating a higher chance of them being a useful predictor and the t-test analysis does not suggest the removal of any features. \n",
    "\n",
    "### Insulin Sub-Analysis\n",
    "\n",
    "While Insulin has a large percent of its values missing, because of its high predictive potential, we will perform a sub-analysis including this feature. We first create a model including this feature and removing all observations where Insulin has a value of 0. \n",
    "\n",
    "and then create a model removing insulin the feature to maintain a larger sample size. We do this to confirm which method will produce a better performing model. \n",
    "\n",
    "To maintain our large sample size, we decided to remove the features from the dataset missing a significant number of values, these features are: Insulin and Skin Thickness. We then removed individual observations with remaining missing values from Glucose, Blood Pressure and BMI. However given Insulin's predictive potential we will perform a sub-analysis including insulin with a smaller sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b48007-92a8-40ef-a7d3-2de720a798d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove observations with missing values of Glucose, Blood Pressure, BME, and Insulin\n",
    "cleaned_data_insulin = data.query('Glucose !=0 & BloodPressure !=0 & BMI !=0 & Insulin !=0').copy()\n",
    "\n",
    "# Remove feature from dataset with substantial missing value: Skin Thickness\n",
    "cleaned_data_insulin.drop(['SkinThickness'], axis = 1, inplace = True)\n",
    "cleaned_data_insulin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f665bc59-ad81-4950-99d1-4dbc4db3b20e",
   "metadata": {},
   "source": [
    "Clearly, maintaining insulin as a feature reduces the sample size of the dataset by almost 50%, (2768 original -> 1339). \n",
    "\n",
    "We will next show the pairwise correlation between all features of the cleaned data set with insulin to see if there is any clear relationship between outcome and any two features of the dataset. Here we are mainly looking to identify relationships between insulin and the remaining features. In the general analysis we will further discuss the remaining features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba90e0-6a60-4cb9-81b6-17c86ffc9215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "matrix = sns.pairplot(\n",
    "    cleaned_data_insulin,\n",
    "    hue = 'Outcome',\n",
    "    palette = colors,\n",
    "    plot_kws = {'alpha' : 0.5,\n",
    "               's' : 10}\n",
    ")\n",
    "\n",
    "# Format matrix\n",
    "matrix._legend.set_title('Diabetes \\nOutcome', prop={'size': 14})\n",
    "matrix._legend.set_bbox_to_anchor((1, 0.9))\n",
    "plt.suptitle(\"Correlation Matrix: Data Features Against Diabetes Outcome (0, 1)\", y = 1.01, fontsize = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdb2d5b-cec0-4bf4-88b1-e43c94ba4850",
   "metadata": {},
   "source": [
    "The correlation matrix shows little relationship between insulin and another other feature besides glucose. In this plot we can see a slight positive linear relationship.\n",
    "\n",
    "We will now split the cleaned data with insulin into training and testing datasets to create a logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9345bbc-e8ec-4fa8-983f-43ee3593c208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fd3e96-fb4a-4d74-9a8f-242267628624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d394b06-7ce5-4dc5-9e75-c4e7d0b1e262",
   "metadata": {},
   "source": [
    "We see a slightly linear, positive relationship between glucose and insuin, however aisde from this there doesn't seem to be any strong correlations between features indicating we aren't at risk of having colinearities in the data. However, there also doesn't seem to be any strong clustering of the data by outcome across any two features, but there does appear to be a strong relationship between an increase and glucose and diabetes outcome across all variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55034092-6cd3-4beb-bc7d-17fece0dccc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b31fbf-c26f-48ff-afa4-0638d976e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit a logistic regression model \n",
    "log_reg_insulin = (\n",
    "    lm.LogisticRegression(penalty = None,  # No regularization\n",
    "                          solver = 'lbfgs',\n",
    "                          max_iter = 1000)\n",
    "    .fit(X = x_train_insulin, y = y_train_insulin)         # Fit the model to training data\n",
    ")\n",
    "# Displaying the model estimates\n",
    "log_df_insulin = pd.DataFrame({\n",
    "    'Term': ['Intercept'] + x_train_insulin.columns.tolist(),\n",
    "    'Estimate': np.concatenate((log_reg_insulin.intercept_, log_reg_insulin.coef_.flatten()), axis = 0).round(3)\n",
    "})\n",
    "log_df_insulin['Odds Ratio'] = np.exp(log_df_insulin['Estimate']).round(3)\n",
    "log_df_insulin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0aa81a-43b1-442a-a7c6-e1a330da8fc1",
   "metadata": {},
   "source": [
    "The odds ratio will tell us how the odds of a diabetes diagnosis change with an increase in the predictor feature, while all other features remain constant. In this analysis, ...\n",
    "\n",
    "Now that we know how impactful each feature is on the model, lets see how accurate the logistic regression model is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d56aeed-87a9-4ec8-b38d-f5a8839a1dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbad636-4dc4-4c17-b994-9cd3c517c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to convert predicted probabilities to binary predictors\n",
    "def threshold_pred(p, t):\n",
    "    return (p >= t) * 1\n",
    "\n",
    "# Predict probabilities for diabetes and convert them to binary predictions\n",
    "log_reg_prob_insulin = log_reg_insulin.predict_proba(X = x_test_insulin)[:,1]\n",
    "\n",
    "# Tune threshold\n",
    "log_reg_predict_insulin = {}\n",
    "log_reg_accuracy_insulin = {}\n",
    "values = np.arange(0.1,1,0.1)\n",
    "for value in values:\n",
    "    v = value.round(2)\n",
    "    log_reg_predict_insulin[f'{v}'] = threshold_pred(log_reg_prob_insulin, v)\n",
    "    log_reg_accuracy_insulin[f'{v}'] = np.mean(log_reg_predict_insulin[str(v)] == np.array(y_test_insulin)).round(4)\n",
    "\n",
    "# Print the accuracy of the logistic regression model \n",
    "for key in log_reg_accuracy_insulin:\n",
    "    print(f'The accuracy of model with threshold {key} is: {log_reg_accuracy_insulin[key]}') \n",
    "    \n",
    "print(f'\\nThe accuracy of predicting every patient as non-diabetic: {1-np.mean(y_test_insulin).round(4)}') \n",
    "\n",
    "# Print the threshold that produces the maximum accuracy\n",
    "max_key_insulin = max(log_reg_accuracy_insulin, key=log_reg_accuracy_insulin.get)\n",
    "print(f'The threshold value which produces the model with the maximum accuracy is {max_key_insulin} with an accuracy of {log_reg_accuracy_insulin[max_key_insulin]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8387f23a-ea04-413a-b326-517f4d09905f",
   "metadata": {},
   "source": [
    "From our calculations, we can see our logistc regression model has decent accuracy, and will predict a patient's diabetes diagnosis 81.12% of the time. This is better than just predicting every patient was not diabetic, which is correct 71.33% of the time.\n",
    "\n",
    "To better understand the accuracy of our model we can create a confusion matrix to understand how often the model is correct, and how often it predicts false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302f12a-826f-4c97-8638-afb8227ac9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a03c67-3fdf-4d93-b9af-fcb557fb58f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316fb2ad-e0f6-4a75-8f82-66e5650c0c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2279f3-b8f6-48b5-906a-2207ea5f21a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbab1cf7-7fb5-4ea6-b2af-1a408b8ca021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786bd2d2-8387-4393-ad98-d939fb15aafe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e5039-1b49-43e1-a1f4-f6bd543cbc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d666b-dde7-45de-b43f-9dfa7ff6eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion matrix with the percent accurately predicted \n",
    "logistic_regression_confusion_matrix_insulin = confusion_matrix(\n",
    "    y_true = y_test_insulin,\n",
    "    y_pred = log_reg_predict_insulin[max_key_insulin], \n",
    "    normalize = 'true'\n",
    ")\n",
    "\n",
    "# Display the confusion matrix\n",
    "sns.heatmap(logistic_regression_confusion_matrix_insulin,\n",
    "           annot = True,\n",
    "           fmt = '.3f',\n",
    "           cmap = 'Blues',\n",
    "           annot_kws = {'size': 16},\n",
    "           xticklabels = ['Non Diabetic','Diabetic'],\n",
    "           yticklabels = ['Non Diabetic','Diabetic'])\n",
    "\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix for Logistic Regression\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a003a2ca-15d0-47cc-9aa7-b9f6eed66fb4",
   "metadata": {},
   "source": [
    "From the confusion matrix we can see that the model correctly predicted non diabetic patients 85.6% of the time. However, it only correctly predicted patients with diabetes 69.9% of the time. Now, we will determine whether removing insulin as a feature improves the accuracy of the logistic regression model. \n",
    "\n",
    "### Complete Analysis\n",
    "\n",
    "Create cleaned dataset removing observations with missing values for Glucose, Blood Pressure and BMI and remove feature: Insulin and Skin Thickness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e390af1a",
   "metadata": {
    "title": "Clean Data"
   },
   "outputs": [],
   "source": [
    "# Remove observations with missing values of Glucose, Blood Pressure, and BMI\n",
    "cleaned_data = data.query('Glucose !=0 & BloodPressure !=0 & BMI !=0').copy()\n",
    "\n",
    "# Remove features from working dataset with substantial missing values: Insulin, Skin Thickness\n",
    "cleaned_data.drop(['Insulin','SkinThickness'], axis = 1, inplace = True)\n",
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8cf6ea-75fc-4610-829f-cf72057b49b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8be2189e-c0ac-42d6-b416-a064fd730789",
   "metadata": {},
   "source": [
    "We will show the pairwise correlation between all features of the cleaned data set to see if there is any clear relationship between outcome and any two features of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35396a3a-f771-449a-8252-a531ea04d5aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "matrix = sns.pairplot(\n",
    "    cleaned_data,\n",
    "    hue = 'Outcome',\n",
    "    palette = colors,\n",
    "    plot_kws = {'alpha' : 0.5,\n",
    "               's' : 10}\n",
    ")\n",
    "\n",
    "# Format matrix\n",
    "matrix._legend.set_title('Diabetes \\nOutcome', prop={'size': 14})\n",
    "matrix._legend.set_bbox_to_anchor((1, 0.9))\n",
    "plt.suptitle(\"Correlation Matrix: Data Features Against Diabetes Outcome (0, 1)\", y = 1.01, fontsize = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc098c-854a-48ce-af76-88a6001bc57e",
   "metadata": {},
   "source": [
    "When looking at the scatter plots we see the same trends as in the previous correlation matrix. We include this again without the inclusion of insulin to highlight the relationships used in the complete analysis.\n",
    "\n",
    "At the end of our exploratory data analysis, we have satesafactorily cleaned the dataset and can move on to build and evaluate potential models\n",
    "\n",
    "## Model Building\n",
    "\n",
    "Prior to building any models, we will split the dataset into testing and training sets. This allows us to determine the accuracy of the model against unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68238aab-e9b7-4baa-892b-c17095151a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test datasets \n",
    "x_train_insulin, x_test_insulin, y_train_insulin, y_test_insulin = train_test_split(\n",
    "    cleaned_data_insulin.drop(columns = 'Outcome', axis = 1),\n",
    "    cleaned_data_insulin.Outcome,\n",
    "    test_size = 0.3, \n",
    "    random_state = 3870)\n",
    "x_train_insulin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6bf6d2-5570-457e-957f-582148b7f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test datasets \n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    cleaned_data.drop(columns = 'Outcome', axis = 1),\n",
    "    cleaned_data.Outcome,\n",
    "    test_size = 0.3, \n",
    "    random_state = 3870)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d7d7d7-ad76-4b87-a826-36d5231497b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test datasets \n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    cleaned_data.drop(columns = 'Outcome', axis = 1),\n",
    "    cleaned_data.Outcome,\n",
    "    test_size = 0.3, \n",
    "    random_state = 3870)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9f317a-45b3-47a2-99f7-13c8841aecf3",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "We chose to start with a logistic regression model to predict diabetes, utilizing the useful features analyzed in the previous section. Logistic regression is a good starting point for a classification model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff924ac-c48c-4ec5-b707-7554ebb71c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit a logistic regression model \n",
    "log_reg = (\n",
    "    lm.LogisticRegression(penalty = None,  # No regularization\n",
    "                          solver = 'lbfgs',\n",
    "                          max_iter = 1000)\n",
    "    .fit(X = x_train, y = y_train)         # Fit the model to training data\n",
    ")\n",
    "# Displaying the model estimates\n",
    "log_df = pd.DataFrame({\n",
    "    'Term': ['Intercept'] + x_train.columns.tolist(),\n",
    "    'Estimate': np.concatenate((log_reg.intercept_, log_reg.coef_.flatten()), axis = 0).round(3)\n",
    "})\n",
    "log_df['Odds Ratio'] = np.exp(log_df['Estimate']).round(3)\n",
    "log_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b1f469-8692-4957-81f5-36541a93a1bd",
   "metadata": {},
   "source": [
    "The odds ratio will tell us how the odds of a diabetes diagnosis change with an increase in the predictor feature, while all other features remain constant. In this analysis, we can see that there is little to no association between blood pressure and a diabetes diagnosis, as its odds ratio is close to 1. In contrast, the Diabetes Pedigree Function shows a stronger association, with a higher odds ratio indicating increased odds of a diabetes diagnosis as this value rises. The other features examined, such as BMI, Glucose, and Pregnancies, demonstrate slight associations with diabetes diagnosis, reflected by odds ratios slightly above 1.\n",
    "\n",
    "Now that we know how impactful each feature is on the model, lets see how accurate the logistic regression model is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c707d42-b8cd-4821-9ff4-500031080aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for diabetes and convert them to binary predictions\n",
    "log_reg_prob = log_reg.predict_proba(X = x_test)[:,1]\n",
    "\n",
    "# Tune threshold\n",
    "log_reg_predict = {}\n",
    "log_reg_accuracy = {}\n",
    "values = np.arange(0.1,1,0.1)\n",
    "for value in values:\n",
    "    v = value.round(2)\n",
    "    log_reg_predict[f'{v}'] = threshold_pred(log_reg_prob, v)\n",
    "    log_reg_accuracy[f'{v}'] = np.mean(log_reg_predict[str(v)] == np.array(y_test)).round(4)\n",
    "\n",
    "# Print the accuracy of the logistic regression model \n",
    "for key in log_reg_accuracy:\n",
    "    print(f'The accuracy of model with threshold {key} is: {log_reg_accuracy[key]}') \n",
    "    \n",
    "print(f'\\nThe accuracy of predicting every patient as non-diabetic: {1-np.mean(y_test).round(4)}') \n",
    "\n",
    "# Print the threshold that produces the maximum accuracy\n",
    "max_key = max(log_reg_accuracy, key=log_reg_accuracy.get)\n",
    "print(f'The threshold value which produces the model with the maximum accuracy is {max_key} with an accuracy of {log_reg_accuracy[max_key]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39589802-a9a9-411a-97d2-8f7e1cd5d91b",
   "metadata": {},
   "source": [
    "From our calculations, we can see our logistc regression model has decent accuracy, and will predict a patient's diabetes diagnosis 77.3% of the time. This is better than just predicting every patient was not diabetic, which is correct 65.82% of the time.\n",
    "\n",
    "To better understand the accuracy of our model we can create a confusion matrix to understand how often the model is correct, and how often it predicts false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c2d7a5-45c7-425f-986a-8cbdc76e15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion matrix with the percent accurately predicted \n",
    "logistic_regression_confusion_matrix = confusion_matrix(\n",
    "    y_true = y_test,\n",
    "    y_pred = log_reg_predict[max_key], \n",
    "    normalize = 'true'\n",
    ")\n",
    "\n",
    "# Display the confusion matrix\n",
    "sns.heatmap(logistic_regression_confusion_matrix,\n",
    "           annot = True,\n",
    "           fmt = '.3f',\n",
    "           cmap = 'Blues',\n",
    "           annot_kws = {'size': 16},\n",
    "           xticklabels = ['Non Diabetic','Diabetic'],\n",
    "           yticklabels = ['Non Diabetic','Diabetic'])\n",
    "\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix for Logistic Regression\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a43fc6-e503-4b51-943c-cf6737246c25",
   "metadata": {},
   "source": [
    "From the confusion matrix we can see that the model correctly predicted non diabetic patients 90.1% of the time. However, it only correctly predicted patients with diabetes 52.6% of the time. This means our model is missing about half of our diabetic patients, which poses significant issues when considering the impacts of missing a diagnosis of diabetes in a patient and prolonging the time until treatment. \n",
    "\n",
    "Considering that our model could be significantly improved in correctly predicting diabetes, we will move towards ensemble methods: Random Forest and XGBoost.\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "First we will perform a grid search to identify the best hyperparameter values before proceeding to creating the Random Forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f2c23e-e1a0-4e7f-bc7e-4ea5a07617a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form the grid of values to choose hyperparameter values from \n",
    "hps_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200, 300, 400, 500],\n",
    "    'max_features': np.arange(1,7, 1).tolist(),\n",
    "    'max_depth': [1, 2, 10, 20, 50, None]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c570f77-78fd-409e-ab92-29954e43e1bf",
   "metadata": {},
   "source": [
    "Perform cross validation to select the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df0efe0-b9b8-4a9a-8e86-4cad54828cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cv partition\n",
    "cv5 = KFold(n_splits = 5, shuffle = True, random_state = 3870)\n",
    "\n",
    "# Create instance of GridSearchCV engine \n",
    "hp_grid_search = GridSearchCV(\n",
    "    ensemble.RandomForestClassifier(random_state = 3870),\n",
    "    hps_grid,\n",
    "    cv = cv5,\n",
    "    scoring = 'accuracy',\n",
    "    n_jobs = -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36813e9e-bad7-4cf4-b3dd-134adf609706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the training data to the GridSearcCV engine \n",
    "hp_grid_search.fit(X = x_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cdf311-59a1-40f4-88d2-ff8514fc2531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameters\n",
    "print(f\"The best choice for the number of trees is {hp_grid_search.best_params_['n_estimators']}\")\n",
    "print(f\"The best choice for the number of features is {hp_grid_search.best_params_['max_features']}\")\n",
    "print(f\"The best choice for max depth is {hp_grid_search.best_params_['max_depth']}\")\n",
    "print(f\"which has an accuracy of: {hp_grid_search.best_score_: .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a359adb-8c2c-4570-ad2e-8a64efc6e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the overall best model\n",
    "best_model = hp_grid_search.best_estimator_\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f4990-3b31-4a59-a55a-45a05cbfe349",
   "metadata": {},
   "source": [
    "After the grid search, we have found the best Random Forest model includes one hundred trees, each with one feature, and a maximum depth of twenty. This model generates an R^2 value of 0.870 on average across five cross-validation folds. This is considered a good model, with significant improvement in predicting diabetes diagnoses than that generated by the logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0221ef88-4a9a-4bc6-a07c-9b7abd7d1483",
   "metadata": {},
   "source": [
    "Now that we have determined the best hyper parameters for our model and fit it to our data to create the best model, we will evaluate the accuracy of the model using our testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf1596-2909-4a5d-9da0-caa31fce930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of best model with train and test sets\n",
    "rf_training_accuracy = best_model.score(x_train, y_train)\n",
    "rf_testing_accuracy = best_model.score(x_test, y_test)\n",
    "print('Training Accuracy: ',round(rf_training_accuracy, 3), ' Testing Accuracy: ',round(rf_testing_accuracy, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec4d85c-ce2c-486f-9457-5947657024bb",
   "metadata": {},
   "source": [
    "The model perfectly classified all cases in the training set, this might suggest overfitting however the model has a 98.5% accuracy on the testing set, indicating it predicts diabetes very well. Next we will examine the confusion matrix for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a99e0-2a77-4f2a-9b59-bbfa98f6ac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the predicted y values from the best model\n",
    "y_pred = best_model.predict(X=x_test)\n",
    "\n",
    "# Create a confusion matrix with the percent accurately predicted \n",
    "random_forest_confusion_matrix = confusion_matrix(\n",
    "    y_true = y_test,\n",
    "    y_pred = y_pred, \n",
    "    normalize = 'true'\n",
    ")\n",
    "\n",
    "# Display the confusion matrix\n",
    "sns.heatmap(random_forest_confusion_matrix,\n",
    "           annot = True,\n",
    "           fmt = '.3f',\n",
    "           cmap = 'Blues',\n",
    "           annot_kws = {'size': 16},\n",
    "           xticklabels = ['Non Diabetic','Diabetic'],\n",
    "           yticklabels = ['Non Diabetic','Diabetic'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix for Random Forest\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ffe6ec-4c3d-406e-b221-bdffc8160e0d",
   "metadata": {},
   "source": [
    "The Random Forests confusion matrix shows much improvement from the logistic regression model. It correctly predicted non-diabetics 98.6% of the time, and diabetics 98.1% of the time. This shows significant improvement in the model and promise that diabetes diagnosis can be predicted by useful known health factors. The modelâ€™s strong accuracy in both categories suggests it could be a valuable tool in early detection and prevention strategies for at-risk populations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70321956-eead-463d-9844-ae433ce4ea21",
   "metadata": {},
   "source": [
    "\n",
    "### XGBoost\n",
    "The second non-parametric model we will use to predict diabetes in patients is XGBoost. First we must determine the best combination of hyper parameters using a grid search in the same way we did earlier but with XGBClassifier this time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827e9edd-f7e6-4c31-93bb-b54511425415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form the grid of values to choose hyper parameter values from \n",
    "\n",
    "xgb_hp_dict = {\n",
    "    'learning_rate':    [0.01, 0.05, 0.1, 0.2, 0.3], # Each new prediction's contribution to y_hat\n",
    "    'n_estimators':     [100, 250, 500, 750, 1000],  # Number of trees to make\n",
    "    'max_depth':        [1, 3, 5, 7],                # Maximum number of splits: 1, 8, 32, 128 leaf nodes   \n",
    "    'colsample_bytree': [0.4, 0.6, 0.8, 1],          # Number of features to use per tree (40, 60, 80, & 100%)\n",
    "    'lambda':           [0, 0.5, 1, 2, 5]            # Regularization parameters to try\n",
    "}\n",
    "\n",
    "# 5 fold cross-val\n",
    "cv5 = KFold(n_splits = 5, shuffle = True, random_state = 3870)\n",
    "\n",
    "# Defining the model:\n",
    "xgb_class = XGBClassifier(random_state = 3870, early_stopping_rounds = 50)\n",
    "\n",
    "# Defining the grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator = xgb_class,\n",
    "    param_grid = xgb_hp_dict,\n",
    "    n_jobs = -1,\n",
    "    cv = cv5    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4a34e-0f95-4f04-9bf5-505a4b98eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conducting the grid search\n",
    "gs_results = (\n",
    "    grid_search\n",
    "    .fit(\n",
    "        x_train, y_train,\n",
    "        eval_set = [(x_test, y_test)],\n",
    "        verbose = False\n",
    "    )\n",
    ")\n",
    "\n",
    "#define the best parameters\n",
    "best_hps = grid_search.best_params_\n",
    "best_hps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d249248-e6b0-49df-aeda-9642799cda44",
   "metadata": {},
   "source": [
    "The best hyperparameters for the XGBoost model has 60% of features for each tree, a L2 regularization strenght of 2, a learning rate of 0.3, a maximum tree depth of 5, and 500 boosting rounds. \n",
    "Next, we will define the best model and fit it with the testing and training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd40f5ce-b149-4589-bfea-9b69b3d1e5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the best model and fit to data\n",
    "xgb_best_model = (\n",
    "    XGBClassifier(\n",
    "        **best_hps,  # Assigns the dictionary values to the parameter keys\n",
    "        random_state = 3870,\n",
    "        early_stopping_rounds = 20\n",
    "    )\n",
    "    .fit(\n",
    "        x_train, y_train,\n",
    "        eval_set = [(x_test, y_test)],\n",
    "        verbose = False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Calculating the accuracy of the model\n",
    "accuracy_xgb_tuned = best_model.score(x_test, y_test)\n",
    "print('Train Accuracy: ',best_model.score(x_train, y_train), ' Test Accuracy: ', accuracy_xgb_tuned.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d15178d-645f-47a5-a519-c53e789b63c4",
   "metadata": {},
   "source": [
    "Again, as expected, the accuracy of the XGBoost model on the training data is 1.0, and on the training data is 0.985. This indicates the accuracy of the XGBoost model is very good, and is equivalent to that of the Random Forest model. \n",
    "\n",
    "To further determine the quality of the model we will make another confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f623919-eceb-47fb-ba5d-0a326227717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the predicted y values from the best model\n",
    "xgb_y_pred = best_model.predict(X = x_test)\n",
    "\n",
    "# Create a normalized confusion matrix of true/ false positives and negatives\n",
    "XGB_confusion_matrix = confusion_matrix(\n",
    "    y_true = y_test,\n",
    "    y_pred = xgb_y_pred, \n",
    "    normalize = 'true'\n",
    ")\n",
    "# Display the confusion matrix\n",
    "sns.heatmap(XGB_confusion_matrix,\n",
    "           annot = True,\n",
    "           fmt = '.3f',\n",
    "           cmap = 'Blues',\n",
    "           annot_kws = {'size': 16},\n",
    "           xticklabels = ['Non Diabetic','Diabetic'],\n",
    "           yticklabels = ['Non Diabetic','Diabetic'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2250e05f-c0b9-4da2-b17d-e60ea566d40c",
   "metadata": {},
   "source": [
    "The XGBoost model has the same accuracy, and predictions as the Random Forest model, indicating that with our data, a more complex model does not inherently make a better model for our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb9b4c-6e3f-452b-99b1-38f144c3d9f9",
   "metadata": {},
   "source": [
    "# Summary and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed9df1-d940-40f0-9fa6-da8d10c55d65",
   "metadata": {},
   "source": [
    "Our project aims to explore if we can predict diabetes diagnoses with known health data, via multiple machine learnign models. We explored parametric and non parametric methods: logistic regression, Random Forests, and XGBoost. Useful factors from our data includes glucose levels, BMI, age, Diabetes Pedigree Function, and number of pregnancies. A sub-analysis identified a positive association between the number of pregnancies and diabetes diagnosis, as well as an inclusive finding that considering those who have never been pregnant or cannot become pregnant was still informative.\n",
    "\n",
    "The first modelling method used was logistic regression, a non-parametric method for binary, categorical outcome variables. This model showed decent accuracy, predicting correctly 77.3% of the time. However, upon further analysis, this model only correctly predicted those without diabetes, and had a high false negative rate of 47.4%. This model would miss almost half of diabetic patients, which could have severe consequences in a medical setting when early intervention, treatment, and diagnosis can impact a patient's wellbeing and quality of life. \n",
    "\n",
    "The second model used was Random Forests, which improved upon the accuracy of the logistic regression model to 98.5%. Upon further analysis of the confusion matrix, the model had significant improvement with the false negative rates remaining of 1.9%. \n",
    "\n",
    "The third model used was XGBoost, which is a more compelx model, however it did not yield a higher accuracy, remaining at 98.5%. The confusion matrix shows a similar trend, with the false positive rate being equivalent to the 1.9% of the Random Forests model. \n",
    "\n",
    "Overall, these models demonstrated the potential of using known health features to accurately predict diabetes in patients, showing promise as a tool for early detection and prevention strategies to improve the quality of life for diabetic patients. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14adca04-9325-4b2d-961b-c34bbfb8008d",
   "metadata": {},
   "source": [
    "# Limitations and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56599888-dc12-46ff-bc77-c7e28e203d9b",
   "metadata": {},
   "source": [
    "The first major limitation is the amount of missing values in the 2-hour serum Insulin column in our data set. This required us to eliminate it as a potential predictive feature, which could have improved the model. In addition, the pregnancy column does not discriminate between those who cnnot get pregnant with those who can, resulting in an over reporting of zero pregnancies, therefore potentially skewing the relationship between diabetes and pregnancies, which required an additional sub analysis. Lastly, future research should also focus on improving data quality by minimizing missing values for critical features. \n",
    "\n",
    "Second, this data set does not include socioeconomic factors, which we know can be highly associated with diabetes in patients. Factors like sex, race, income, and housing status could all impact the predictive ability of a model for diabetes. \n",
    "\n",
    "Third, the data set has a high sample size which creates large test statistics and small p-values, which can potentially inflate the statistical significance of the difference of groups in the two sample t-tests. \n",
    "\n",
    "In addition, using machine learning as a predictive tool in diagnostic techniques is still heavily debated in the medical field. Our project highlights the limitations of model building, where the potential for missed diagnoses is high. \n",
    "\n",
    "Future recommendations include increasing the number of socioeconomic factors recorded for each patient, and explore other clinical study types to best explore the diagnosis. A cohort study would provide more perspective to the features that impact diabetes by providing a timeframe of when a patient was diagnosed with diabetes, and may illude to what features changed leading to that diagnosis\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
